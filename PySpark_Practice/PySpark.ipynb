{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ada6c80-8786-40c8-af7a-7af8a19aa2a3",
   "metadata": {},
   "source": [
    "# Codecademy: Practice with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5ae47-f503-4052-8c63-edbd77ac918f",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e21f7a0",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928ce4d",
   "metadata": {},
   "source": [
    "1. Start a SparkSession and assign it the name `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121e41ee",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x106292bd0>\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# confirm your session is built\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e68472",
   "metadata": {},
   "source": [
    "2. Change the list `student_data` into an RDD called `student_rdd` with 5 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e47b66f",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 0.72, 'CA'),\n",
       " ('Jake', 1555, 0.83, 'NY'),\n",
       " ('Cody', 1439, 0.92, 'CA'),\n",
       " ('Lisa', 1442, 0.81, 'FL'),\n",
       " ('Daniel', 1600, 0.88, 'TX'),\n",
       " ('Kelvin', 1382, 0.99, 'FL'),\n",
       " ('Nancy', 1442, 0.74, 'TX'),\n",
       " ('Pavel', 1599, 0.82, 'NY'),\n",
       " ('Josh', 1482, 0.78, 'CA'),\n",
       " ('Cynthia', 1582, 0.94, 'CA')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "student_rdd = spark.sparkContext.parallelize(student_data, 5)\n",
    "\n",
    "# confirm your RDD contains the correct data\n",
    "student_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ac139",
   "metadata": {},
   "source": [
    "3. Check the number of partitions for `student_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f42d161",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "student_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c1c0a-04f2-44d4-ad56-c9a5cf65f173",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758f1483-ea77-4ce2-bd24-048e87039c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_rdd = spark.sparkContext.parallelize(student_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0cebd-66dc-41af-9980-1c60b29cc4c4",
   "metadata": {},
   "source": [
    "1. Convert the grades in the third column from decimals to whole numbers (multiply by 100) and keep the other three variables. Save this new RDD as `rdd_transformation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e6a069-ae06-4d8d-8a9a-a9d2cd25fbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72.0, 'CA'),\n",
       " ('Jake', 1555, 83.0, 'NY'),\n",
       " ('Cody', 1439, 92.0, 'CA'),\n",
       " ('Lisa', 1442, 81.0, 'FL'),\n",
       " ('Daniel', 1600, 88.0, 'TX'),\n",
       " ('Kelvin', 1382, 99.0, 'FL'),\n",
       " ('Nancy', 1442, 74.0, 'TX'),\n",
       " ('Pavel', 1599, 82.0, 'NY'),\n",
       " ('Josh', 1482, 78.0, 'CA'),\n",
       " ('Cynthia', 1582, 94.0, 'CA')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], x[2]*100, x[3]))\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_transformation.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf8a3f-fc75-4681-8b80-6aa1e5ee7fc1",
   "metadata": {},
   "source": [
    "2. Filter `rdd_transformation` to just those rows with grades above 80 and save the new RDD as `rdd_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84a78b1c-a049-4bc6-9367-6d8679c0ca67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jake', 1555, 83.0, 'NY'),\n",
       " ('Cody', 1439, 92.0, 'CA'),\n",
       " ('Lisa', 1442, 81.0, 'FL'),\n",
       " ('Daniel', 1600, 88.0, 'TX'),\n",
       " ('Kelvin', 1382, 99.0, 'FL'),\n",
       " ('Pavel', 1599, 82.0, 'NY'),\n",
       " ('Cynthia', 1582, 94.0, 'CA')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "rdd_filtered = rdd_transformation.filter(lambda x: x[2]>80)\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f6a00-9c22-4400-8722-40816abc6396",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de2c277b-5253-4674-b62b-1f1dba74fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_rdd = spark.sparkContext.parallelize(student_data)\n",
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb1018-71f5-46b1-b0ac-7b49146de098",
   "metadata": {},
   "source": [
    "1. View the first 5 elements of `rdd_transformation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b31a8e-ef6b-436d-a978-f566618d46b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72, 'CA'),\n",
       " ('Jake', 1555, 83, 'NY'),\n",
       " ('Cody', 1439, 92, 'CA'),\n",
       " ('Lisa', 1442, 81, 'FL'),\n",
       " ('Daniel', 1600, 88, 'TX')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "rdd_transformation.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77cda7-5df7-4fdf-85e9-87fae3a01450",
   "metadata": {},
   "source": [
    "2. Sum the grades in `rdd_transformation` and save the result as `sum_gpa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc54d09-d701-4319-b846-ab1a7be03f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sum_gpa = rdd_transformation.map(lambda x: x[2]).reduce(lambda x, y: x+y)\n",
    "\n",
    "# view the sum\n",
    "sum_gpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5b441-f335-4a10-9806-829f3e4595f1",
   "metadata": {},
   "source": [
    "3. Divide `sum_gpa` by `rdd_transformation.count()` to get the average grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4d4fe1-f245-4e1e-ac36-966a4c505c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sum_gpa / rdd_transformation.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269a69d-f17a-4fb1-abec-ba3f24d214e7",
   "metadata": {},
   "source": [
    "### Associative and Commutative Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eac16f9-0cf2-4d01-905e-d94372449485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafac79c-ecdd-4ec5-b837-b2fb2c3b1486",
   "metadata": {},
   "source": [
    "1. Run the provided code. What do you notice about the result of the summation as the number of partitions grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "240607e0-1b2f-4466-8dff-03bf33b6dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition:  [[1, 2, 3, 4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1, 2], [3, 4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1], [2, 3], [4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1], [2], [3], [4, 5]]\n",
      "addition:  15\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "for i in range(1,5):\n",
    "    rdd = spark.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('addition: ', rdd.reduce(lambda a,b: a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ce215-6060-490a-bf8a-fed7e2ab9b91",
   "metadata": {},
   "source": [
    "2. Run the provided code. What do you notice about the result of the division as the number of partitions grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2badfe27-dcf0-41b9-a349-df7e5c3f4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition:  [[1, 2, 3, 4, 5]]\n",
      "division:  0.008333333333333333\n",
      "partition:  [[1, 2], [3, 4, 5]]\n",
      "division:  3.3333333333333335\n",
      "partition:  [[1], [2, 3], [4, 5]]\n",
      "division:  1.875\n",
      "partition:  [[1], [2], [3], [4, 5]]\n",
      "division:  0.20833333333333331\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    rdd = spark.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('division: ', rdd.reduce(lambda a,b: a/b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316f52c-7ccb-4c90-8f3a-2a7b9c46012b",
   "metadata": {},
   "source": [
    "Notice in the output that no matter how our list is being partitioned, the sum is still 15, but the division operation has different solutions based on the partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8882c76-9505-4284-9f7d-a35b1c7508d4",
   "metadata": {},
   "source": [
    "### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "becc7c56-e024-481f-9031-83d56ff67cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "student_rdd = spark.sparkContext.parallelize(student_data)\n",
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e404d67-68fe-4632-b67d-f555cd0786c5",
   "metadata": {},
   "source": [
    "1. Broadcast the `states` dictionary to Spark Cluster. Save this object as `broadcastStates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa3a6b1f-bd50-43d7-a892-ea1c107ed7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.broadcast.Broadcast"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "# confirm type\n",
    "type(broadcastStates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3638c8-dce5-4d90-bc65-5d44156410b2",
   "metadata": {},
   "source": [
    "2. Reference `broadcastStates` to map the two-letter abbreviations to their full names. Save transformed rdd as `rdd_broadcast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ae4a80-0647-4c53-8c43-79bae73d5020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72, 'California'),\n",
       " ('Jake', 1555, 83, 'New York'),\n",
       " ('Cody', 1439, 92, 'California'),\n",
       " ('Lisa', 1442, 81, 'Florida'),\n",
       " ('Daniel', 1600, 88, 'Texas'),\n",
       " ('Kelvin', 1382, 99, 'Florida'),\n",
       " ('Nancy', 1442, 74, 'Texas'),\n",
       " ('Pavel', 1599, 82, 'New York'),\n",
       " ('Josh', 1482, 78, 'California'),\n",
       " ('Cynthia', 1582, 94, 'California')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2],broadcastStates.value[x[3]]))\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_broadcast.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc1939-bd4e-4b8b-8eda-4952c5344de5",
   "metadata": {},
   "source": [
    "### Accumulator Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d48d7ba-509a-421e-ad5f-01ad21146177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "student_rdd = spark.sparkContext.parallelize(student_data)\n",
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2],broadcastStates.value[x[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed88031-3fb3-4e6e-9ce0-4083d52ebf31",
   "metadata": {},
   "source": [
    "1. Create the accumulator variable that starts at 0 and name it `sat_1500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6be6f2ef-7aed-4b8e-9e83-2cf674606844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.accumulators.Accumulator"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sat_1500 = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# confirm type\n",
    "type(sat_1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85d6ed-254a-4e17-be79-ddee7e2a3ccd",
   "metadata": {},
   "source": [
    "2. Create a function called `count_high_sat_score` that increments our accumulator by 1 whenever it encounters a score of over 1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97d0c1bf-0905-4c2e-af84-83122c158e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function count_high_sat_score at 0x1072c6a20>\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "def count_high_sat_score(x):\n",
    "    if x[1]>1500: sat_1500.add(1)\n",
    "\n",
    "# confirm saved as a function\n",
    "print(count_high_sat_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b0a0b-ffdd-4986-811b-6207f97dadc2",
   "metadata": {},
   "source": [
    "3. Call `count_high_sat_score` in an action that will apply the function to each element in `rdd_broadcast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "511ff3b7-e7b7-417f-ab32-fcdf89d1ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "rdd_broadcast.foreach(lambda x: count_high_sat_score(x))\n",
    "\n",
    "# confirm accumulator worked\n",
    "print(sat_1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf6cc9-0014-4167-a22a-3f7ee835c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
